
# Introduction

The aim of the project is to build a model which takes 8 inputs and gives an output of 2 variables. The
model predicts Cooling and Heating Load based on the parameters of the building. All buildings have the
same volume but different dimensions, are built from the same material. Input values are: "Relative
Compactness", "Surface Area", "Wall Area", "Roof Area", "Overall Height", "Orientation", "Glazing area",
"Glazing area distribution". In the report and in the code naming (x1, x2, x3.., y1, y2) is used for
simplicity.

- X1 = Relative Compactness
- X2 = Surface Area",
- X3 = "Wall Area
- X4 = Roof Area",
- X5 = "Overall Height
- X6 = Orientation
- X7 = Glazing area
- X8 = Glazing area distribution
- Y1 = Cooling
- Y2 = Heating

# Procedure

The code is written in Python 3.72 (Worked on lab machines), using OOP design structure. Contains
main file which runs all application, also Data class which is responsible for the data processing and train
which is responsible for training. Samples were all correct, so there was no need to delete any row.

## Loading data

Data is loaded in the main file, in the method called _loadData()_. As the data was in csv file, it was read
using pandas _read_csv_ function.


## Data processing

After the data is read it needs to be processed and cleaned if needed, and this is done in _Data.py_ class.
Initialiser of the Data class saves and stores dataframe object generated by Pandas, also it splits input
and output and stores their values in numpy array, which is used later for plotting graphs.

To see the all picture, scatter matrix is plotted for each feature using pandas scatter_matrix function:


After the analysis it was achieved that the X1 and X2 have some patter in common, as they illustrate the
same graph but in reflection.

After some analysis it is identified that some features, particularly x1 or x2 and x4 or x5 can be dropped,
because they are correlated. This is possible from the correlation matrix.

This is more visible from the plotted correlation matix, which is plotted using Seaborn library:


From the figure, it is obvious that X1 and X2 (Relative Compactness, Surface Area) are correlated. Same
with the values of X4 and X5(Roof Area, Overall Height). Both variables are inversely correlated, which
makes since, as the volume is constant, the roof area should get smaller when the overall height
increases.

### Split the data

To train the model the data was split into training and testing, using pandas train_test_split method. The
method splitDataToTrainAndTest() returns the tupple of 4 variables: train_x, train_y, test_x, test_y. For
testing 35% of all dataset was taken, using th pandas train_test_split method, which picks random data
points from dataset. There was no need of the validation set till some point.

## Training

First training method is Linear Regression. If plot all input-output graphs, the linear regression is not the
best model for this dataset. ï¿¼



The graphs show that linear regression will not give very accurate result. Moreover, the graphs for Y
and Y2 are very similar, this can be justified by correlation matrix from the section before, as both
outputs are linearly correlated. Hence, the results for both outputs will be similar after each training.

## Linear Regression

Mean squared error for the linear regression is : 9.44.

Accuracy acquired using r2_score was 89%.

To see the model, it was plotted on top of the scatter plot of the dataset. However, as there are lots of
inputs, it is not obvious how does the model look like in 6 dimensions.

Due to this, another graph was plotted, scatter plot, where the actual datapoints and predicted ones are
on the same graph. So ,it is obvious if the model is doing everythin right.


In fact, the accuracy of the trainig data (seen data) should be higher, but as soon as the model is not
doing well, both of the graphs achieve medium quality. It is obvious that in some points model cannot
predict some data.

## Polynomial regression


Unfortunately, it is not possible to visualize all inputs together and we do not see if the data follows the
linear pattern. As soon as the linear regression gives high error, it is possible to increase the order of the
equation and make it polynomial regression.

In case of polynomial regression, it was required to choose the order of the polynomial, for simplicity it
was assigned to 4 at the beginning. By transforming their input data into the higher order polynomial,
this was performed again using linear regression.

Polynomial regression with order 4 gave the MSE of 1.439, which is almost 7 times better.

Here, it is more visible that the model does worse on the testing set, however it is also obvious that it
predicts much better than the linear regression of the order 1.


If zoom the testing data input output graphs, the model will look is not so accurate.

## Finding the best order for the polynomial

Order 4 predicted everything very well, but is it enough, or is it the maximum performance, is still a
question. To find this out, simple for loop was written which iterated over polynomial orders starting
from 1 to 9.

```
Order- 1 MSE = 9.
Order- 2 MSE = 5.
Order- 3 MSE = 2.
Order- 4 MSE = 1.
Order- 5 MSE = 1.
Order- 6 MSE = 2.
Order- 7 MSE = 2.
Order- 8 MSE = 3.
Order- 9 MSE = 4.
Best Order- 4 MSE = 1.
```
The best order which gave minimum error was polynomial with the order 4.

## Evaluation - Cross Validation

The evaluation method cross validation is used.

In cross validation the data needs to be split in equal sized subsets, in our case 50. It should be trained
on 49 and 1 set should be used for test. All this procedure needs to be run 50 times with altering the
testing data. And the result should be averaged at the end.


Was not the best idea to use this approach, in fact for some reason it was giving very high error on one
part of the data.

Average MSE = 2.

It should be noted that the reason of MSE being so low, is that because it uses polynomial regression.

# Conclusion

The data was processed, correlation between features was analysed. Some features were eliminated.

Linear regression was applied to predict output, the order of polynomial was increased, the best degree
was found. Also, cross validation was applied to train the model.



